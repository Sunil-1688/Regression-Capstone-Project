# -*- coding: utf-8 -*-
"""Yes Bank Stock Closing Price Prediction(Sunil Yadav).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ppIuXoLlUbS0jqWubJnNSGT0JMpuO8Mp

# **Project Name :Yes Bank Stock Closing Price Prediction**

##### **Project Type**    - EDA/Regression/Classification/Unsupervised
##### **Contribution**    - Individual/Team
##### **Team Member 1 -**

# **Project Summary -**

To predict the monthly closing price of Yes Bank stock using historical stock data with machine learning models. The goal is to support better decision-making for investors or analysts using predictive modeling.

# **GitHub Link -**

Provide your GitHub Link here.

# **Problem Statement**

The goal of this project is to predict the monthly closing price of Yes Bank stock using historical stock data (Open, High, Low prices). By applying machine learning models like Linear Regression we aim to forecast future prices accurately and support informed investment decisions.

# **General Guidelines** : -

1.   Well-structured, formatted, and commented code is required.
2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.
     
     The additional credits will have advantages over other students during Star Student selection.
       
             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go
                       without a single error logged. ]

3.   Each and every logic should have proper comments.
4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.
        

```
# Chart visualization code
```
            

*   Why did you pick the specific chart?
*   What is/are the insight(s) found from the chart?
* Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

5. You have to create at least 15 logical & meaningful charts having important insights.


[ Hints : - Do the Vizualization in  a structured way while following "UBM" Rule.

U - Univariate Analysis,

B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)

M - Multivariate Analysis
 ]





6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.


*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.


*   Cross- Validation & Hyperparameter Tuning

*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.

# ***Let's Begin !***

## ***1. Know Your Data***

### Import Libraries ###
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from sklearn.linear_model import Lasso, Ridge
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error,mean_absolute_percentage_error
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import VotingRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import PolynomialFeatures

"""### Dataset Loading"""

# Import csv file and put the data in data variable name
df = pd.read_csv('/content/drive/MyDrive/dataset/data_YesBank_StockPrices.csv', encoding = 'unicode_escape')

"""### Dataset First View"""

# Dataset First Look
df.head()

"""### Dataset Rows & Columns count"""

# Dataset Rows & Columns count
df.shape

"""### Dataset Information"""

# Dataset Info
df.info()

"""#### Duplicate Values"""

# Dataset Duplicate Value Count
len(df[df.duplicated()])

"""#### Missing Values/Null Values"""

# Missing Values/Null Values Count
df.isnull().sum()

# Visualizing the missing values
plt.figure(figsize=(12, 6))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title("Missing Values Heatmap")
plt.show()

"""### What did you know about your dataset?

The dataset contains 185 rows and 5 columns, representing monthly stock market records for Yes Bank.

Each row corresponds to a unique month, summarizing the stock's activity during that period.

## ***2. Understanding Your Variables***
"""

# Dataset Columns
df.columns

# Dataset Describe
df.describe()

"""### Variables Description

Answer Here

### Check Unique Values for each variable.
"""

# Check Unique Values for each variable.
for col in df.columns:
    unique_vals = df[col].nunique()
    print(f"{col}: {unique_vals} unique values")

"""## 3. ***Data Wrangling***

### Data Wrangling Code
"""

# Write your code to make your dataset analysis ready.
# Write your code to make your dataset analysis ready.

# Step 1: Preview Data
print("First 5 rows:\n", df.head())
print("\nData Types:\n", df.dtypes)

# Step 2: Rename Columns (if needed)
df.columns = df.columns.str.strip().str.capitalize()

# Step 3: Convert 'Date' column to datetime
df['Date'] = pd.to_datetime(df['Date'], format='%b-%y', errors='coerce')

# Step 4: Check for missing values
print("\nMissing values:\n", df.isnull().sum())

# Step 5: Drop rows with missing values (or handle as needed)
df.dropna(inplace=True)

# Step 6: Sort by date
df.sort_values('Date', inplace=True)

# Step 7: Reset index
df.reset_index(drop=True, inplace=True)

# Step 8: Ensure numeric columns are in correct type
numeric_cols = ['Open', 'High', 'Low', 'Close']
df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')

# Optional: Check for duplicates
duplicates = df.duplicated().sum()
print(f"\nDuplicate rows: {duplicates}")
df.drop_duplicates(inplace=True)

# Final preview
df.info()
df.describe()
df.head()

"""### What all manipulations have you done and insights you found?

Answer Here.

## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***

#### Average Closing Price of Yes Bank by Year Chart -1
"""

# Chart - 1 visualization code

df['Date'] = pd.to_datetime(df['Date'], format='%b-%y', errors='coerce')
df = df.dropna(subset=['Date'])

# Group by year and compute average closing price
df['Year'] = df['Date'].dt.year
avg_close_by_year = df.groupby('Year')['Close'].mean()

# Plot in your preferred pastel style
plt.figure(figsize=(10, 4))
avg_close_by_year.plot(kind='bar', color=sns.color_palette('pastel'))

plt.title('Average Closing Price of Yes Bank by Year')
plt.ylabel('Average Closing Price (₹)')
plt.xlabel('Year')
plt.show()

"""##### 1. Why did you pick the specific chart?

**Grouping by year reduces noise from daily volatility and makes it easier to spot long-term growth or decline.**

##### 2. What is/are the insight(s) found from the chart?

**The chart helps pinpoint the exact years where performance shifted—e.g., from growth to decline.**

**This can be correlated with real-world events (e.g., RBI interventions, leadership changes).**

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**By identifying years with strong performance, management can analyze what strategies worked and replicate them.**


**Suggests loss of investor confidence, poor financial performance, or corporate governance issues.**

#### Highest & Lowest Stock Prices with Dates Chart - 2
"""

# Chart - 2 visualization code

df['Date'] = pd.to_datetime(df['Date'], format='%b-%y', errors='coerce')
df = df.dropna(subset=['Date'])

# Find max and min close prices and their dates
max_close = df.loc[df['Close'].idxmax()]
min_close = df.loc[df['Close'].idxmin()]

# Create a DataFrame for plotting
price_data = pd.Series({
    f"Highest ({max_close['Date'].strftime('%b-%Y')})": max_close['Close'],
    f"Lowest ({min_close['Date'].strftime('%b-%Y')})": min_close['Close']
})

# Plot using pastel color palette
plt.figure(figsize=(6, 4))
price_data.plot(kind='bar', color=sns.color_palette('pastel'))

plt.title('Highest and Lowest Closing Prices of Yes Bank')
plt.ylabel('Closing Price (₹)')
plt.xlabel('Price Type')
plt.show()

"""##### 1. Why did you pick the specific chart?

**A bar chart clearly displays the magnitude of the highest and lowest prices side-by-side, making it easy to compare at a glance.**

##### 2. What is/are the insight(s) found from the chart?

**The highest closing price represents a time when investor sentiment was at its peak — possibly due to strong financials, growth projections, or favorable economic conditions.**

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Understanding the peak performance periods allows decision-makers to analyze what worked during those times (like strong earnings or positive investor sentiment) and replicate those success factors.**


**Identifying periods of sharp decline (e.g., 2020 crash during Yes Bank crisis) might highlight historical mismanagement or financial instability, which could concern investors.**

#### Price volatility Chart - 3
"""

# Chart - 3 visualization code

df['Date'] = pd.to_datetime(df['Date'], format='%b-%y', errors='coerce')
df = df.dropna(subset=['Date'])

# Calculate yearly volatility (standard deviation of Close)
df['Year'] = df['Date'].dt.year
a = df.groupby('Year')['Close'].std()

# Plot with your style
plt.figure(figsize=(9, 3))
a.plot(kind='bar', color=sns.color_palette('pastel'))
plt.title('Year-wise Price Volatility of Yes Bank')
plt.ylabel('Close Price Volatility (₹)')
plt.xlabel('Year')
plt.show()

"""##### 1. Why did you pick the specific chart?

**Bar charts are excellent for year-wise comparisons, making it easy to visualize how much the stock price fluctuated each year.**

##### 2. What is/are the insight(s) found from the chart?

**Some years (e.g., 2019–2020) show sharp spikes in volatility, indicating periods of high market uncertainty or crisis, such as the Yes Bank financial turmoil and moratorium period.**

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.
"""



"""**Understanding when and why volatility spiked helps investors and fund managers assess risk and build more resilient portfolios.**


**This period significantly impacted shareholder wealth, investor trust, and resulted in negative stock performance.**

#### Moving Average Trends 3-Year Chart - 4
"""

# Chart - 4 visualization code

df['Date'] = pd.to_datetime(df['Date'], format='%b-%y', errors='coerce')
df = df.dropna(subset=['Date'])

# Extract year and calculate average closing price per year
df['Year'] = df['Date'].dt.year
yearly_avg = df.groupby('Year')['Close'].mean()

# Compute 3-year moving average
moving_avg = yearly_avg.rolling(window=3).mean()

# Plot using your pastel bar chart style
plt.figure(figsize=(9, 3))
moving_avg.plot(kind='bar', color=sns.color_palette('pastel'))
plt.title('3-Year Moving Average of Yes Bank Closing Price')
plt.ylabel('Moving Avg Close Price (₹)')
plt.xlabel('Year')
plt.show()

"""##### 1. Why did you pick the specific chart?

**Daily or monthly stock prices often show short-term fluctuations due to market noise. A 3-year moving average smooths this out and highlights the long-term price trend, making it easier to interpret.**

##### 2. What is/are the insight(s) found from the chart?

**The 3-year moving average reveals whether the stock has been in a consistent uptrend, downtrend, or sideways trend over the years. For Yes Bank, the chart may show a sharp decline during critical years (e.g., 2018–2020), followed by stabilization or minor recovery.**

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Investor Confidence & Strategy:**
**A gradually recovering trend in the moving average can restore investor confidence, encouraging long-term investments in the stock or related financial instruments.**


**Downward Sloping Moving Average (e.g., 2018–2020):**
**A significant decline in the moving average during this period highlights underlying financial instability, poor governance, or external market shocks.**

#### Top 3 Yearly % Growth in Closing Price Chart - 5
"""

# Chart - 5 visualization code
growth = df.groupby('Year')['Close'].mean().pct_change() * 100

# Get top 3 years with highest growth
top_growth = growth.sort_values(ascending=False).head(3)

# Plot
plt.figure(figsize=(9, 3))
top_growth.plot(kind='bar', color=sns.color_palette('pastel'))
plt.title('Top 3 Years with Highest % Growth in Closing Price')
plt.xlabel('Year')
plt.ylabel('% Growth in Avg Closing Price')
plt.show()

"""##### 1. Why did you pick the specific chart?

**It pinpoints specific years of exceptional performance, helping investors and analysts understand when the stock gained strong market momentum.**

##### 2. What is/are the insight(s) found from the chart?

**The chart highlights the three years with the highest percentage growth in Yes Bank’s closing prices, signaling strong investor confidence or favorable market conditions during those periods.**

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Identifying years with highest growth allows stakeholders to analyze successful policies or market conditions that led to the spike.**


**The absence of certain years in the top-3 indicates flat or declining prices, hinting at market distrust, poor performance, or economic stress.**

#### Monthly Closing Price Trends Chart - 6
"""

# Chart - 6 visualization code
# Convert Period to string format
df['Month'] = df['Date'].dt.to_period('M').astype(str)

a = df.groupby('Month')['Close'].mean()

plt.figure(figsize=(14, 4))
a.plot()
plt.title('Monthly Average Close Price')
plt.xlabel('Month')
plt.ylabel('Average Close Price (₹)')
plt.show()

"""##### 1. Why did you pick the specific chart?

**Stock prices can fluctuate a lot on a daily basis due to various short-term factors. Using the monthly average close price smooths out these daily variations, giving a clearer picture of the overall trend or movement in price.**

##### 2. What is/are the insight(s) found from the chart?

**Upward Trend: If the monthly average close prices are consistently rising, this suggests that the asset has been increasing in value over time.**

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Investment Decisions: Investors or business leaders can make more informed decisions on when to buy, hold, or increase their position in an asset.**

**Decreased Consumer Confidence: A falling price might signal to consumers that the product, company, or asset is not performing well, which could hurt sales and brand reputation.**

#### Trend Classification (Uptrend/Downtrend per Year) Chart - 7
"""

# Chart - 7 visualization code
# Group data and compute trends
a = df.groupby('Year')['Close'].agg(['first', 'last'])
a['Trend'] = a.apply(lambda row: 'Uptrend' if row['last'] > row['first'] else 'Downtrend', axis=1)

# Rename columns for clarity
a_plot = a[['first', 'last']]
a_plot.columns = ['Start of Year', 'End of Year']

# Plot using pastel colors
plt.figure(figsize=(14, 5))
a_plot.plot(kind='bar', color=sns.color_palette('pastel'))

plt.title('Yearly Opening vs Closing Prices')
plt.xlabel('Year')
plt.ylabel('Stock Price (₹)')
plt.show()

"""##### 1. Why did you pick the specific chart?

**The yearly opening and closing prices allow for an analysis of long-term trends. By looking at the first and last price of the year, it offers a broader perspective of how the asset performed over a full year, which helps smooth out daily or monthly fluctuations.**

##### 2. What is/are the insight(s) found from the chart?

**If the closing price is higher than the opening price, it indicates positive annual performance (profit/growth).**

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**If the chart shows consistent yearly growth (closing prices higher than opening prices), businesses and investors can gain confidence to invest more capital or expand operations.**


**Missed Recovery Signals**

**If a business ignores a year that closes positively after multiple down years, it may miss a market recovery signal, leading to lost opportunities for early positioning.**

#### Correlation Heatmap Chart - 8
"""

# Correlation Heatmap visualization code
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.heatmap(df[['Open', 'High', 'Low', 'Close']].corr(), annot=True, cmap='Blues', linewidths=0.5)
plt.title("Correlation Matrix")
plt.show()

"""##### 1. Why did you pick the specific chart?

**Positive correlation (close to +1) means both variables increase together.**

**Negative correlation (close to -1) means one increases while the other decreases.**

**A value near 0 means little or no correlation**

##### 2. What is/are the insight(s) found from the chart?

**Variables that increase or decrease together show strong positive correlation.**


**Insight Example: If “Sales Revenue” and “Marketing Spend” show a correlation of +0.85, it suggests that increasing marketing likely boosts revenue**

#### Pair Plot Chart - 9
"""

# Pair Plot visualization code
sns.pairplot(df)

"""##### 1. Why did you pick the specific chart?

**A pair plot displays scatter plots between every pair of variables, making it easy to spot patterns or correlations at a glance.**


**It helps to identify linear or non-linear relationships between variables.**

##### 2. What is/are the insight(s) found from the chart?

Answer Here

## ***6. Feature Engineering & Data Pre-processing***

### 1. Handling Missing Values
"""

# Check missing values
df.isnull().sum()
# Fill or drop
df.fillna(method='ffill', inplace=True)
# or df.dropna(inplace=True)

"""#### What all missing value imputation techniques have you used and why did you use those techniques?

### 2. Handling Outliers

**Correlation & scatter plot**
"""

# Define numeric features manually if not done yet
numeric_features = ['Open', 'High', 'Low']

# Scatter plots with correlation and regression line
for col in numeric_features:
    fig = plt.figure(figsize=(9, 3))
    ax = fig.gca()

    feature = df[col]
    label = df['Close']
    correlation = feature.corr(label)

    plt.scatter(x=feature, y=label, alpha=0.6)
    plt.xlabel(col)
    plt.ylabel('Close Price')
    ax.set_title(f'Close Price vs {col} - Correlation: {correlation:.2f}')

    # Regression line
    z = np.polyfit(feature, label, 1)
    y_hat = np.poly1d(z)(feature)
    plt.plot(feature, y_hat, "r--", lw=1)
    plt.show()

"""##### What all outlier treatment techniques have you used and why did you use those techniques?

Answer Here.

### 3. Categorical Encoding
"""

# Encode your categorical columns
#df = pd.get_dummies(df,drop_first=True,sparse=True)
df.info()

df['Date'] =pd.to_datetime(df['Date'],format= '%d/%m/%Y')
df['day_of_week'] = df['Date'].dt.day_name()
df['month_name'] = df['Date'].dt.month_name()
df['year_name'] = df['Date'].map(lambda x: x.year).astype("object")

df.drop(['Date'],axis = 1,inplace = True)

df.head()

df['week'] = df['day_of_week'].apply(lambda x:'Weekend'  if x=='Saturday' or  x== 'Sunday' else 'Weekdays')

df.head()

df.info()

plt.figure(figsize=(15, 5))
sns.pointplot(x=df["month_name"], y=df["Close"], hue=df["week"])
plt.title('Monthly Trend of Yes Bank Closing Price by Week Type')
plt.ylabel('Closing Price')
plt.xlabel('Month')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

int_columns_df = df.select_dtypes(include=['int', 'float'])
int_columns_df

"""#### What all categorical encoding techniques have you used & why did you use those techniques?

Answer Here.

### 4. Textual Data Preprocessing
(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)

#### 1. Expand Contraction
"""





"""#### 2. Lower Casing"""

# Lower Casing

"""#### 3. Removing Punctuations"""

# Remove Punctuations

"""#### 4. Removing URLs & Removing words and digits contain digits."""

# Remove URLs & Remove words and digits contain digits

"""#### 5. Removing Stopwords & Removing White spaces"""

# Remove Stopwords

# Remove White spaces

"""#### 6. Rephrase Text"""

# Rephrase Text

"""#### 7. Tokenization"""

# Tokenization

"""#### 8. Text Normalization"""

# Normalizing Text (i.e., Stemming, Lemmatization etc.)

"""##### Which text normalization technique have you used and why?

Answer Here.

#### 9. Part of speech tagging
"""

# POS Taging

"""#### 10. Text Vectorization"""

# Vectorizing Text

"""##### Which text vectorization technique have you used and why?

Answer Here.

### 4. Feature Manipulation & Selection

#### 1. Feature Manipulation
"""

# Manipulate Features to minimize feature correlation and create new features

"""#### 2. Feature Selection"""

# Select your features wisely to avoid overfitting

"""##### What all feature selection methods have you used  and why?

Answer Here.

##### Which all features you found important and why?

Answer Here.

### 5. Data Transformation

#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?
"""

# Transform Your data

"""### 6. Data Scaling"""

# Scaling your
from sklearn.preprocessing import StandardScaler

# Select numeric features to scale (excluding 'Close' if it's your target)
features_to_scale = ['Open', 'High', 'Low']  # update as needed

scaler = StandardScaler()
df_scaled = df.copy()
df_scaled[features_to_scale] = scaler.fit_transform(df[features_to_scale])

print(df_scaled.head())

"""##### Which method have you used to scale you data and why?

### 7. Dimesionality Reduction
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Select numeric features
features = ['Open', 'High', 'Low']  # update this based on your dataset
X = df[features]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA
pca = PCA(n_components=2)  # reduce to 2 dimensions for visualization
X_pca = pca.fit_transform(X_scaled)

# Convert to DataFrame (optional)
pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])

print(pca_df.head())

"""##### Do you think that dimensionality reduction is needed? Explain Why?

Answer Here.
"""

# DImensionality Reduction (If needed)

"""##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)

Answer Here.
"""



"""### 8. Data Splitting"""

# Split your data to train and test. Choose Splitting ratio wisely.
from sklearn.model_selection import train_test_split

# Step 1: Define features and target
X = df[['Open', 'High', 'Low']]  # update based on available columns
y = df['Close']

# Step 2: Split into train and test sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Optional: check shapes
print("Train shape:", X_train.shape, y_train.shape)
print("Test shape:", X_test.shape, y_test.shape)

"""##### What data splitting ratio have you used and why?

Answer Here.

## ***7. ML Model Implementation***

### ML Model - 1
"""

model = LinearRegression()
model.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score

# Predict on train and test sets
train_pred = model.predict(X_train)
test_pred = model.predict(X_test)

# Train metrics
print('Train Mean Squared Error:', mean_squared_error(y_train, train_pred))
print('Train Mean Absolute Error:', mean_absolute_error(y_train, train_pred))
print('Train Mean Absolute Percentage Error:', mean_absolute_percentage_error(y_train, train_pred) * 100)
print('Train R2 Score:', r2_score(y_train, train_pred))

print('\n' + '*' * 80 + '\n')

# Test metrics
print('Test Mean Squared Error:', mean_squared_error(y_test, test_pred))
print('Test Mean Absolute Error:', mean_absolute_error(y_test, test_pred))
print('Test Mean Absolute Percentage Error:', mean_absolute_percentage_error(y_test, test_pred) * 100)
print('Test R2 Score:', r2_score(y_test, test_pred))



# Appending all models parameters to the corrosponding list
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score


def score_metrix (model,X_train,X_test,Y_train,Y_test):

  '''
    train the model and gives mae, mse,rmse,r2,adj r2 score of the model

  '''
  #training the model
  model.fit(X_train,Y_train)

  # Training Score
  training  = model.score(X_train,Y_train)
  print("Training score  =", training)



  #predicting the Test set and evaluting the models

  if model == LinearRegression() or model == Lasso() or model == Ridge():
    Y_pred = model.predict(X_test)

    #finding mean_absolute_error
    MAE  = mean_absolute_error(Y_test**2,Y_pred**2)
    print("MAE :" , MAE)

    #finding mean_squared_error
    MSE  = mean_squared_error(Y_test**2,Y_pred**2)
    print("MSE :" , MSE)

    #finding root mean squared error
    RMSE = np.sqrt(MSE)
    print("RMSE :" ,RMSE)

    #finding the r2 score

    r2 = r2_score(Y_test**2,Y_pred**2)
    print("R2 :" ,r2)
    #finding the adjusted r2 score
    adj_r2=1-(1-r2_score(Y_test**2,Y_pred**2))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))
    print("Adjusted R2 : ",adj_r2,'\n')

  else:
    # for tree base models
    Y_pred = model.predict(X_test)

    #finding mean_absolute_error
    MAE  = mean_absolute_error(Y_test,Y_pred)
    print("MAE :" , MAE)

    #finding mean_squared_error
    MSE  = mean_squared_error(Y_test,Y_pred)
    print("MSE :" , MSE)

    #finding root mean squared error
    RMSE = np.sqrt(MSE)
    print("RMSE :" ,RMSE)

    #finding the r2 score

    r2 = r2_score(Y_test,Y_pred)
    print("R2 :" ,r2)
    #finding the adjusted r2 score
    adj_r2=1-(1-r2_score(Y_test,Y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))
    print("Adjusted R2 : ",adj_r2,'\n')



  print('*'*80)
  # print the cofficient and intercept of which model have these parameters and else we just pass them
  try :
    print("coefficient \n",model.coef_)
    print('\n')
    print("Intercept  = " ,model.intercept_)
  except:
    pass
  print('\n')
  print('*'*20, 'ploting the graph of Actual and predicted only with 80 observation', '*'*20)

  # ploting the graph of Actual and predicted only with 80 observation for better visualisation which model have these parameters and else we just pass them
  try:
    # ploting the line graph of actual and predicted values
    plt.figure(figsize=(15,7))
    plt.plot((Y_pred)[:80])
    plt.plot((np.array(Y_test)[:80]))
    plt.legend(["Predicted","Actual"])
    plt.show()
  except:
    pass

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.

#### **LinearRegression**
"""

# Visualizing evaluation Metric Score chart
score_metrix(LinearRegression(),X_train,X_test,y_train,y_test)

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# Cross-validation score
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

pipe = make_pipeline(StandardScaler(), Lasso(max_iter=10000))

parameters = {'lasso__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 20, 50]}

lasso_cv = GridSearchCV(pipe, parameters, cv=5, scoring='r2')
lasso_cv.fit(X_train, y_train)

score_metrix(lasso_cv, X_train, X_test, y_train, y_test)

"""##### Which hyperparameter optimization technique have you used and why?

Lasso Regression has a critical hyperparameter: alpha, which controls the degree of regularization (penalty on large coefficients).

Choosing the right alpha directly affects:

Model accuracy

Overfitting or underfitting

Hence, GridSearchCV was used to:

Systematically search over a predefined alpha grid.

Use 5-fold cross-validation to evaluate model performance on each fold.

Select the alpha value that gives the best average performance (R² score in your case).

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

While the numerical improvements are subtle, the real value of using Lasso with cross-validation is in:

Reducing overfitting

Improving generalization

Providing a more robust and stable model

### ML Model - 2

#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.

### **Using Random Forest Regressor**
"""

# Visualizing evaluation Metric Score chart
score_metrix(RandomForestRegressor(),X_train,X_test,y_train,y_test)

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

L1 = Lasso(max_iter=10000)  # Increase max_iter to ensure convergence
parameters = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 20, 50]}  # Reasonable range

lasso_cv = GridSearchCV(L1, parameters, cv=5, scoring='r2')  # Or use MSE or MAE as needed
lasso_cv.fit(X_train, y_train)

score_metrix(lasso_cv, X_train, X_test, y_train, y_test)

"""##### Which hyperparameter optimization technique have you used and why?

Random Forest Regressor and GridSearchCV is a brute-force method for hyperparameter tuning that tests all possible combinations of specified parameters. It performs cross-validation for each combination, selecting the best one based on a scoring metric (in your case, 'r2' score). Here's why it was a suitable choice:

Systematic Search: Exhaustively searches across the specified parameter grid.

Cross-Validation: Reduces overfitting and improves generalization.

Best Parameters: Automatically selects the optimal hyperparameters.

Ease of Integration: Easily integrates with pipelines (e.g., with make_pipeline for Lasso).

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

Technique Used: GridSearchCV with 5-fold cross-validation.

Best Model: Lasso Regression performed better overall across all key metrics.

Update Notes: Even though Random Forest shows a slightly higher training score, Lasso offers better generalization (lower errors and higher R² on test data).

#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used.

### ML Model - 3

### **KNN Regressor**
"""

# ML Model - 3 Implementation

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

# Visualizing evaluation Metric Score chart
score_metrix(KNeighborsRegressor(),X_train,X_test,y_train,y_test)

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)
# Create the model with increased iterations for better convergence
L1 = Lasso(max_iter=10000)

# Define more reasonable alpha values (remove extremely small ones)
parameters = {
    'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 20, 50]
}

# Perform Grid Search with Cross-Validation
lasso_cv = GridSearchCV(L1, parameters, cv=5)

# Fit the model
lasso_cv.fit(X_train, y_train)

# Now evaluate
score_metrix(lasso_cv, X_train, X_test, y_train, y_test)

"""##### Which hyperparameter optimization technique have you used and why?

Systematic Search: GridSearchCV exhaustively searches over a specified parameter grid (in this case, different values of alpha for Lasso regression).

Validation Through Cross-Validation: It performs cross-validation (cv=5), which helps ensure the model generalizes well to unseen data and prevents overfitting.

Optimal Hyperparameter Tuning: By evaluating all combinations, it identifies the best-performing set of hyperparameters.

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

1. Mean Absolute Error (MAE)
Definition: Average of absolute differences between predicted and actual values.

Why It Matters:

MAE is easy to interpret in the same unit as the target variable.

It tells you, on average, how much your predictions are off from the actual values.

Business Impact:

Useful when every error has a consistent cost (e.g., demand forecasting, cost prediction).

Lower MAE = more stable and predictable business planning.

2. Mean Squared Error (MSE)
Definition: Average of squared differences between predicted and actual values.

Why It Matters:

Penalizes larger errors more heavily, highlighting outliers or bad predictions.

Business Impact:

Important in scenarios where larger errors are exponentially more costly (e.g., pricing errors, medical costs).

Helps reduce severe mispredictions that can result in major business losses.

3. Root Mean Squared Error (RMSE)
Definition: Square root of MSE, brings the error back to the original scale.

Why It Matters:

Makes the interpretation of MSE easier.

More sensitive to large errors than MAE.

Business Impact:

Helps assess the overall predictive accuracy.

Key in high-stakes environments where extreme prediction failures must be minimized.

4. R² Score (Coefficient of Determination)
Definition: Proportion of variance in the dependent variable that is predictable from the independent variables.

Why It Matters:

Measures how well the model explains the variability of the output.

Business Impact:

High R² indicates strong predictive power, useful for strategic decisions.

R² near 1 means the model can be trusted for decision support (e.g., revenue forecasting, inventory control).

5. Adjusted R²
Definition: Modified version of R² that adjusts for the number of predictors in the model.

Why It Matters:

Prevents overfitting by penalizing unnecessary variables.

Business Impact:

Ensures the model is generalizable and not overly complex, which is essential for scalable business solutions.

### 1. Which Evaluation metrics did you consider for a positive business impact and why?

Better Overall Accuracy:

The Lasso model achieved lower error values (MAE, MSE, RMSE) than KNN, indicating better predictions.

High Predictive Power:

An R² score of 0.9904 means the model explains over 99% of the variance, which is excellent for regression tasks.

Regularization (Feature Selection):

Lasso also performs automatic feature selection, helping reduce overfitting and improving model interpretability.

Cross-Validation Confidence:

GridSearchCV with 5-fold cross-validation ensures the model is robust and generalizes well to unseen data.

### 2. Which ML model did you choose from the above created models as your final prediction model and why?

Better Overall Accuracy:

The Lasso model achieved lower error values (MAE, MSE, RMSE) than KNN, indicating better predictions.

High Predictive Power:

An R² score of 0.9904 means the model explains over 99% of the variance, which is excellent for regression tasks.

Regularization (Feature Selection):

Lasso also performs automatic feature selection, helping reduce overfitting and improving model interpretability.

Cross-Validation Confidence:

GridSearchCV with 5-fold cross-validation ensures the model is robust and generalizes well to unseen data.

Business Stability:

Lower errors translate to more consistent and reliable forecasts, which is critical for business decisions like pricing, resource allocation, and budgeting.

### 3. Explain the model which you have used and the feature importance using any model explainability tool?

Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that adds L1 regularization to the cost function.

Key Characteristics:
Penalizes the absolute value of the coefficients.

Can shrink some coefficients to exactly zero, making it useful for feature selection.

Helps in reducing overfitting and improving model interpretability.

Works well when there is multicollinearity among features.

# **Conclusion**

In this project, we developed a machine learning model to predict the closing price of Yes Bank's stock using historical data and various financial indicators. We employed techniques such as data preprocessing, feature engineering, and model evaluation using cross-validation to ensure the robustness of our predictions.

Initially, a Linear Regression model was implemented to establish a baseline. While this model provided a straightforward approach, we further experimented with regularized models like Ridge and Lasso Regression to address potential overfitting and improve generalization.

After performing cross-validation and hyperparameter tuning using GridSearchCV, we found that the Ridge regression model with an optimized alpha parameter yielded better predictive performance compared to the standard Linear Regression model. Evaluation on the test set demonstrated that the model could reasonably capture the trend of the stock's closing prices, with acceptable error metrics such as Mean Squared Error (MSE) and a satisfactory R-squared value

### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***
"""